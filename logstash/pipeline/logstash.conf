input {

	file {
		path => "/data/spark-logs/*/eventlog/*/*/eventlog*"
		codec => "json"
		mode => "read"
		start_position => "beginning"
		file_chunk_size => 32768000
	}
}

filter {
	mutate {
		remove_field => ["spark.executor.extraClassPath",
				 "spark.executor.extraJavaOptions",
				 "spark.executor.extraClassPath",
				 "spark.hadoop.spark.driverproxy.customHeadersToProperties",
				 "spark.hadoop.fs.abfs.impl",
				 "host"]
		# since time is in nanoseconds
		gsub => [
			"time", "\d{6}$", ""
		]
	}
	mutate {
		convert => {
			"Task Info.Accumulables.Update" => "string"
			"Stage Info.Accumulables.Value" => "string"
		}
		lowercase => ["Event"]
	}
	date {
		match => ["Timestamp", "UNIX_MS"]
		remove_field => ["Timestamp"]
	}
	date {
		match => ["time", "UNIX_MS"]
		remove_field => ["time"]
	}
	date {
		match => ["Submission Time", "UNIX_MS"]
		remove_field => ["Submission Time"]
	}
	date {
		match => ["Task Info.Launch Time", "UNIX_MS"]
	}
	date {
		match => ["Stage Info.Submission Time", "UNIX_MS"]
	}
}

output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "elastic"
		password => "changeme"
		ecs_compatibility => disabled
		index => "%{Event}-%{+YYYY.MM.dd}"
	}
}
